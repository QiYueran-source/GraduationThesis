# 基于强化学习的改进方案

## 一、核心设计理念

### 1.1 决策机制设计

**决策格式：**
- 从预测收益率改为量化决策深度
- 决策值范围：`-10.0 ~ 10.0`（连续值）
- 正数表示买入，负数表示卖出，绝对值表示决策强度

```python
# 决策输出
decision = {
    "decision_depth": float  # -10.0 到 10.0
}

# 奖励函数
reward = actual_return * decision_depth
```

**设计优势：**
1. 简单直接：单一连续值，易于 RL 训练
2. 方向明确：正负表示买卖方向，绝对值表示强度
3. 奖励直观：`收益率 × 决策深度` 自然鼓励正确且坚定的决策
4. 适合 AED：不同 agent 的决策差异可直接用于计算 AED

### 1.2 与其他项目的对比

| 项目 | 决策格式 | 复杂度 | 特点 |
|------|---------|--------|------|
| **ai-hedge-fund** | `{action: "buy/sell/hold", quantity: int, confidence: int}` | 中等 | 离散动作 + 数量 + 置信度 |
| **TwinMarket** | `{action: "buy/sell/hold", target_position: float, target_price: float}` | 较高 | 离散动作 + 仓位 + 价格 |
| **我们的设计** | `{decision_depth: float}` | 低 | 单一连续值，最简单 |

## 二、信念优化方案

### 2.1 核心理解

**关键原则：**
- 信念 = LLM 自由生成的文本（不是从参数转换的）
- 优化对象 = LLM 生成信念的能力
- 决策 = 基于信念，LLM 生成的输出

**错误方式（避免）：**
```python
# ❌ 错误：定义参数，然后格式化
belief_params = {"relation": [...], "attention": [...]}
belief_text = format_params_to_text(belief_params)  # 人为格式化
```

**正确方式：**
```python
# ✅ 正确：让 LLM 直接生成信念文本
belief_text = llm.generate("请生成你的投资信念...")
# 信念是 LLM 自由生成的文本，不是从参数转换的
```

### 2.2 优化方案

#### 方案A：直接优化 LLM（如果可训练）

```python
class BeliefRLAgent:
    def __init__(self):
        # LLM 模型（可训练，如 HuggingFace）
        self.llm = load_trainable_model("Qwen2.5-1.5B")
        
        # 生成信念的 prompt（固定模板）
        self.belief_prompt = """
        基于以下因子数据和历史信息，请生成你的投资信念。
        信念应该包括：
        1. 你对因子-收益率关系的理解
        2. 你对不同因子类别的关注度排序
        """
    
    def generate_belief(self, factors, history):
        """LLM 生成信念（自由文本）"""
        prompt = f"{self.belief_prompt}\n因子：{factors}\n历史：{history}"
        belief_text = self.llm.generate(prompt)
        return belief_text
    
    def generate_decision(self, belief, factors):
        """基于信念生成决策"""
        prompt = f"""
        你的投资信念：{belief}
        当前因子数据：{factors}
        请基于你的信念，输出决策深度（-10 到 10）：
        """
        decision = float(self.llm.generate(prompt))
        return decision
    
    def update_with_rl(self, belief, decision, reward):
        """使用 RL 优化 LLM 生成信念的能力"""
        # 使用 REINFORCE 算法
        log_prob = self._compute_log_prob(belief, factors)
        loss = -reward * log_prob  # 负奖励作为损失
        self.llm.backward(loss)
        self.llm.step()
```

#### 方案B：优化生成信念的 Prompt（LLM 不可训练时）

```python
class PromptOptimizedBeliefAgent:
    def __init__(self):
        # LLM 模型（固定，不可训练，如 API）
        self.llm = load_model("Qwen2.5-1.5B")
        
        # 可优化的 prompt 模板
        self.belief_prompt_template = """
        基于以下信息，生成你的投资信念：
        {optimization_hints}
        
        因子数据：{factors}
        历史信息：{history}
        """
        
        # 优化提示（可优化参数）
        self.optimization_hints = {
            "focus_areas": "重点关注动量因子和价值因子",
            "reasoning_style": "使用定量分析",
            "risk_attitude": "中等风险偏好"
        }
    
    def generate_belief(self, factors, history):
        """生成信念"""
        prompt = self.belief_prompt_template.format(
            optimization_hints=self._format_hints(self.optimization_hints),
            factors=factors,
            history=history
        )
        belief_text = self.llm.generate(prompt)
        return belief_text
    
    def update_optimization_hints(self, belief, decision, reward):
        """使用 LLM 根据奖励反馈优化提示"""
        update_prompt = f"""
        你之前的优化提示：{self.optimization_hints}
        
        生成的信念：{belief}
        做出的决策：{decision}
        奖励：{reward}
        
        如果奖励为正，保持或强化这些提示；
        如果奖励为负，调整这些提示。
        
        输出更新后的优化提示：
        """
        updated_hints = self.llm.generate(update_prompt)
        self.optimization_hints = self._parse_hints(updated_hints)
```

## 三、完整训练流程

### 3.1 训练循环

```python
class BeliefRLTraining:
    def train_episode(self, factors, history, actual_return):
        """一个训练回合"""
        
        # Step 1: LLM 生成信念（自由文本）
        belief = self.llm.generate(
            f"{self.belief_prompt}\n因子：{factors}\n历史：{history}"
        )
        # 示例信念：
        # "我认为动量因子与收益率正相关，价值因子与收益率负相关。
        #  我重点关注动量因子类别，其次是价值因子类别。"
        
        # Step 2: 基于信念生成决策
        decision = self.llm.generate(
            f"信念：{belief}\n因子：{factors}\n输出决策深度："
        )
        decision = float(decision)  # -10.0 ~ 10.0
        
        # Step 3: 计算奖励
        reward = actual_return * decision
        
        # Step 4: 优化 LLM 生成信念的能力
        self._update_belief_generation(reward, belief, decision, factors)
        
        return belief, decision, reward
    
    def _update_belief_generation(self, reward, belief, decision, factors):
        """优化信念生成（核心）"""
        if self.llm.is_trainable:
            # 方法A：直接优化 LLM 参数
            log_prob = self._compute_log_prob(belief, factors)
            loss = -reward * log_prob
            self.llm.backward(loss)
        else:
            # 方法B：优化生成信念的 prompt
            self._update_prompt_with_feedback(reward, belief, decision)
```

### 3.2 奖励函数设计

```python
def calculate_reward(decision_depth, actual_return):
    """
    奖励函数：收益率 × 决策深度
    
    设计逻辑：
    - 如果预测正确（收益率和决策同号），奖励为正
    - 如果预测错误（收益率和决策异号），奖励为负
    - 决策深度越大，奖励/惩罚的幅度越大
    """
    base_reward = actual_return * decision_depth
    
    # 可选：添加信念质量奖励
    # belief_quality_reward = -prediction_error
    # total_reward = base_reward + alpha * belief_quality_reward
    
    return base_reward
```

### 3.3 梯度计算（如果 LLM 可训练）

```python
def compute_gradient(self, belief, decision, reward, factors):
    """计算信念生成的梯度"""
    
    # 方法1：有限差分法
    epsilon = 1e-5
    gradient = 0
    
    # 扰动 LLM 参数（简化示例）
    for param in self.llm.parameters():
        param_plus = param + epsilon
        
        # 计算扰动后的信念和决策
        belief_plus = self._generate_belief_with_params(param_plus, factors)
        decision_plus = self._generate_decision(belief_plus, factors)
        
        # 计算梯度
        gradient += (decision_plus - decision) / epsilon
    
    return gradient * reward  # 乘以奖励（REINFORCE 风格）
```

## 四、异质性设计

### 4.1 因子数据异质性

```python
class HeterogeneousAgent:
    def __init__(self, factor_categories):
        """
        每个 agent 只能访问特定的因子类别
        模拟不同投资者的信息能力差异
        """
        self.accessible_categories = factor_categories  # 例如：["momentum", "value"]
        self.factor_mask = self._create_mask(factor_categories)
    
    def filter_factors(self, all_factors):
        """只返回可访问的因子"""
        return {k: v for k, v in all_factors.items() 
                if self._is_accessible(k)}
```

### 4.2 心理特征异质性

```python
class PsychologicalTraits:
    """心理特征（固定设置，异质性的来源）"""
    def __init__(self):
        self.optimism_level = "optimistic"  # optimistic/neutral/pessimistic
        self.rationality_level = "rational"  # rational/random/emotional
        self.risk_preference = "medium"  # high/medium/low
    
    def influence_belief_generation(self, base_prompt):
        """心理特征影响信念生成"""
        prompt = f"""
        {base_prompt}
        
        你的心理特征：
        - 乐观程度：{self.optimism_level}
        - 理性程度：{self.rationality_level}
        - 风险偏好：{self.risk_preference}
        
        请根据这些特征生成你的投资信念。
        """
        return prompt
```

## 五、AED 计算

### 5.1 基于决策差异的 AED

```python
def calculate_aed(agent_decisions):
    """
    计算预期协同分散度（AED）
    
    基于不同 agent 的决策差异
    """
    decisions = [agent.decision_depth for agent in agent_decisions]
    
    # 方法1：决策方差
    aed_variance = np.var(decisions)
    
    # 方法2：决策分歧度
    aed_disagreement = np.std(decisions)
    
    # 方法3：信念差异（如果可用）
    # aed_belief = calculate_belief_diversity(agent_beliefs)
    
    return aed_variance
```

## 六、技术实现要点

### 6.1 LLM 选择

- **训练阶段**：使用 HuggingFace 的可训练模型（如 Qwen2.5-1.5B）
- **推理阶段**：可以使用 Ollama 进行快速推理
- **混合方案**：HuggingFace 训练，Ollama 推理

### 6.2 训练策略

```python
# 阶段1：预训练信念生成器（可选）
for window in training_windows:
    belief = belief_generator(factors)
    predicted_return = predict_from_belief(belief, factors)
    loss = mse_loss(predicted_return, actual_return)
    belief_generator.backward(loss)

# 阶段2：固定信念生成器，训练决策策略（RL）
belief_generator.eval()  # 冻结信念生成器
for window in training_windows:
    belief = belief_generator(factors)
    decision = policy_network(belief, factors)
    reward = calculate_reward(decision, actual_return)
    policy_network.update(reward)

# 阶段3：联合微调（可选）
# 同时更新信念和决策
```

### 6.3 Token 管理

- **不使用历史对话**：每次独立生成，避免 token 累积
- **信念压缩**：将信念总结为关键信息，减少 token
- **因子筛选**：根据 attention 只传递重要因子

## 七、关键优势

1. **简单高效**：单一连续决策值，易于 RL 训练
2. **自然合理**：信念是 LLM 自由生成，符合 LLM 特性
3. **可解释性强**：信念是文本，决策基于信念，易于理解
4. **适合 AED**：不同 agent 的信念和决策差异可直接用于计算
5. **异质性支持**：通过因子访问限制和心理特征实现异质性

## 八、待解决问题

1. **梯度计算**：如果 LLM 可训练，如何高效计算梯度？
2. **Prompt 优化**：如果 LLM 不可训练，如何有效优化 prompt？
3. **信念稳定性**：如何保证信念更新不会过于跳跃？
4. **训练效率**：如何加速训练过程？
5. **多 Agent 并行**：如何高效实现多 agent 并行训练？

